<!DOCTYPE html>

<html>
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  <title>My Little Projects</title>
  <LINK href="styles.css" rel="stylesheet" type="text/css">
</head>

<body>



<!-- Change this code here by copy and pasting your template on line 15 -->
<header class="site-header">

  <div class="wrap title-wrap">
    <a class="site-title" href="/">Machine Learning examples</a>
  </div>

</header>
 <div class="page-content">
      <div class="wrap">
      <div class="post">

  <header class="post-header">
    <h1></h1>
  </header>

  <article class="post-content">
  <p>These are few examples of how ML can be used for predictions</p>
<p>Data is abundant now and is being collected constantly. Machine learning uses concepts of statistics and 
	algorithms are written to see if any sense can be made of the data or if there is any pattern which can be learned. 
	This learning can then be used to probably predict what the next data might be.<br>
The data could be movies that someone loves to watch .Using a well-trained algorithm, one could make 
	an educated guess on what the person might want to see next.
	The data could be images or even text /words that someone uses often in their emails or text messages.<br>

Below are some projects which use machine learning to predict outcomes for future occurrences of the same kind of events.</p>
	  <h2><a href = #kickstarter >ML Model to make your campaign <span style="color: #ff0000;">Successful!!</span></h2>
	  <h2><a href = #parkinsons >ML Model to predict if a person has <span style="color: #ff0000;">Parkinson's</span></a></h2>
	   
<div id = "kickstarter">
	<h2>ML Model to make your campaign</h2>
<p><a href="http://kickstarter.com">Kickstarter</a> is a crowdfunding platform with a community of more than 10 million 
	people who are creative, tech enthusiasts.<br>

Kickstarter works on all or nothing basis: a campaign is launched with a certain amount an artist or developer wants to raise. 
	If it doesn’t meet its goal, the project owner gets nothing. For example: if a projects’ goal is $5000, 
	even if it gets funded till $4999, the project won’t be a success.<br>

If you have any project in mind to post to Kickstarter, the following classifier might help to 
	predict the success of your project being funded or not.<br><br></p>
	<p>The main steps that followed for building a predictive model are</p>
	<p><ul>
	<li>Extracting data</li>
	<li>Exploratory data analysis</li>
	<li>Feature engineering</li>
	<li>Building a model</li>
	</ul></p>
This is an iterative process because data has to re-examined and model rebuilt after tuning the parameters and processing the features until the best fit is found. During this iterative process, one must not sacrifice the simplicity for complexity.
<h2><span style="color: #800000;">Exploratory Data Analysis</span></h2>
Understanding the data is very vital before moving on to classifying or evaluating it. This process is called 
	      Exploratory Data Analysis (EDA). This important step enables one to spot anomalies 
	      
	      
before beginning the Machine Learning model. It also brings out the bigger picture of the data. 
	      
	      EDA could be done in a Graphical or Non-Graphical means. Non-Graphical methods are generally the summary statistics.
	      EDA could also include methods to
<ul>
	<li>Reduce Dimensionality: Reduce the number of features to a linear combination of data so that data has fewer features (Columns) which can explain most of the variance.</li>
	<li>Analyse the Cluster: Data can be organised into clusters so that similar observations are clubbed into clusters.</li>
</ul>
<h2><span style="color: #800000;">Reading and uploading data</span></h2>
Python is the languaged used to analyse and build the model.The following libraries are imported
<ul>
	<li>pandas: for easy manipulation of data structures.</li>
	<li>NumPy: For scientific computations</li>
	<li>sklearn : The machine learning library of Python</li>
</ul>
<div class="fig figcenter fighighlight">
<img class="alignnone size-full wp-image-87" src="https://vkwritings.files.wordpress.com/2020/01/lib.png" alt="lib" width="360" height="82" /></div>

<p>The data used for this evaluation has been downloaded and stored as a  stored as a CSV file which is the commonly used format for exchanging data. To begin with, read the CSV file into a variable.</p>

<div class="fig figcenter fighighlight">
	<img class="alignnone size-full wp-image-88" src="https://vkwritings.files.wordpress.com/2020/01/rawdf.png" alt="rawdf" width="661" height="61" /></div>

To build a model,  the projects that have already been evaluated have to be studied. In order to do that we divide the whole raw data into two data frames. One of which to used to model the prediction model and the other to test if our model is accurate.

The variable /column named ‘Evaluation_set’ is used to identify if the project has been evaluated or not. The Data Frame named ‘modelling_df’ will be is used to build the model.
<h3><img class="alignnone size-full wp-image-81" src="https://vkwritings.files.wordpress.com/2020/01/3.png" alt="3" width="777" height="65" /></h3>
<h3>Check Context of data</h3>
The dataframe which is used for modelling has 50,000 instances or sample projects and 27 features or columns describing various details about the project. The target is the column named ‘State’ which is either a 1 or 0 based on if the campaign was successful/unsuccessful.

<div class="fig figcenter fighighlight">
	<img class="alignnone size-full wp-image-82" src="https://vkwritings.files.wordpress.com/2020/01/4.png" alt="4" width="299" height="119" /></div>

Out of the 27 columns/features the data types of the features have to be checked. A check needs to done to see how many have valid values in them.

<div class="fig figcenter fighighlight"><img class="alignnone size-full wp-image-83" src="https://vkwritings.files.wordpress.com/2020/01/5.png" alt="5" width="941" height="173" /></div>
	      Most of the columns are numerical but a few are non-numerical. Since an ML model cannot be built on non-numerical column one needs to now study on how the columns can be made numerical or eliminated altogether.

The feature named ‘ Country’ describes which country the project belongs to. There are a few unique values. It is definitely a categorical column.
<div class="fig figcenter fighighlight"><img class="alignnone size-full wp-image-84" src="https://vkwritings.files.wordpress.com/2020/01/8.png" alt="8" width="941" height="159" /></div>
<h3>Check for null values and fetaures having single value or outliers</h3>
There are many columns which have many nulls or values which predominantly have only one value or many outliers. An initial check could be made to see which columns to retain and columns that could be dropped. This can be done using statically data or graphically plotting the values

The above function is used to check the distinct value counts, nulls, mean, standard deviation etc of all the columns and decision can be taken on what columns could be retained.
<div class="fig figcenter fighighlight"><img class="alignnone size-full wp-image-85" src="https://vkwritings.files.wordpress.com/2020/01/9.png" alt="9" width="799" height="252" /></div>
<h2><span style="color: #800000;">Preprocessing</span></h2>
Machine learning algorithms require inputs to be numerical and so if there are any columns which are categorical, they need to somehow be transformed into numerical before using them to predict a model.

One of the common ways to handle categorical columns is ‘One Hot Encoding’. This technique can be employed when the values have no particular ordering. For every unique value of the column , another column is created where the value is 1 if for that instance the feature takes that value else 0. For example, the column ‘country’ could be the US. So, a new column (Country_US) can be created which will have a value 1 or 0 based on if the value of the column is ‘US’ or not.

We could use a column transformer to convert all the categorical columns but since we have only one column named ‘Country’ we could use a function.
<div class="fig figcenter fighighlight"><img class="alignnone size-full wp-image-86" src="https://vkwritings.files.wordpress.com/2020/01/10.png" alt="10" width="941" height="259" /></div>
<h3>Dropping columns ahve redundant or missing data</h3>
The features  - currency symbol, trailing code and currency can be dropped. The features ‘is_backing’, ‘permissions’, ‘friends’ and ‘is_starred’ have many missing values, so they could also be dropped at this moment.

The column ‘URLs’ and ‘source_url’ contain only URLs and modelling algorithm needs numeric data, so they could be dropped.

There are a few columns which are JSON string which has to be looked in separately to extract only the columns which add value to the modelling.

The columns to be retained are identified and a new data frame created containing only the columns to be retained.
<div class="fig figcenter fighighlight"><img class="alignnone size-full wp-image-100" src="https://vkwritings.files.wordpress.com/2020/01/29.png" alt="29" width="941" height="164" /></h3>
<h3>Reduction of the feature set</div>
Some of the columns are redundant and some of them can be clubbed to become one column.

To compare the various projects, it would be better to convert the ‘money to be raised’ to a single currency. Since the USD rate is given for the various currencies, a new column is created ‘goal_in_usd’.

<div class="fig figcenter fighighlight"><img class="alignnone size-full wp-image-101" src="https://vkwritings.files.wordpress.com/2020/01/30.png" alt="30" width="941" height="114" />
	      </div>To make sense of the dates we could club the various dates and create single columns containing the various duration in milliseconds.

<div class="fig figcenter fighighlight"><img class="alignnone size-full wp-image-90" src="https://vkwritings.files.wordpress.com/2020/01/13.png" alt="13" width="941" height="80" />
	      </div>
	      Some of the columns from the multiple columns stored in the JSON string can be extracted and added the data frame on which modelling is performed. Looking at the ‘category’ column, the value of column ‘slug’ could be split to obtain the ‘parent category’ and the ‘category column’.

These columns are categorical columns so they could be encoded too.
<div class="fig figcenter fighighlight"><img class="alignnone size-full wp-image-91" src="https://vkwritings.files.wordpress.com/2020/01/14.png" alt="14" width="941" height="304" /></div>
<h3>Scaling</h3>
From the data, it is very obvious that the columns have various units. In some machine learning algorithms, the units have to be scaled because certain weights may update faster than others. Algorithms like Decision tree or k-nearest neighbours are not affected by the scaling.

There are a few methods which can be used to scale features. Some of them are the StandardScaler, MinMaxScaler, RobustScaler and Normalizer.

MinMaxScaler is used in this case and it follows the following formulae

<div class="fig figcenter fighighlight"><img class="alignnone size-full wp-image-92" src="https://vkwritings.files.wordpress.com/2020/01/15.png" alt="15" width="216" height="91" /></div>

The range of the features is now shrunk to values between 0 and 1. If there were negative values the scaler shrinks them to values between 1 and -1. However, this scaler is sensitive to outliers.

<div class="fig figcenter fighighlight"><img class="alignnone size-full wp-image-102" src="https://vkwritings.files.wordpress.com/2020/01/31.png" alt="31" width="941" height="79" /></div>

After the first round of preprocessing, we have 169 columns.
<ul>
	<li>All columns are scaled between 0 and 1.</li>
	<li>Category columns have been one hot encoded</li>
	<li>All datetime columns have been transformed so that we have logical columns which contain some period in seconds.</li>
	<li>Columns which have more than 50% as null values or missing values are dropped.</li>
	<li>All objects which are non-numerical and irrelevant at this stage are dropped.</li>
	<li>No custom transformations have been applied.</li>
</ul>
<h2><span style="color: #800000;">Modelling</span></h2>
There are many algorithms that can be used. Since this project requires a prediction on the state of the project which is either a 1 or 0(Successful/ Unsuccessful), it is considered as a classification problem. Logistic Regression and Decision Tree are two good models to build and test.
<h3>DECISION TREE</h3>
The decision tree is the first model that will be fitted. The data is divided into smaller datasets based on a certain features until the target variable (‘State’) falls in one of the 2 categories (0,1)

GridSearch is used so that the hyperparameters can be tuned to get the most optimal value for the model. The GridSearchCV of the sklearn library is used. A cross-validation process is performed for achieving better results. In K-Fold cross-validation, a given dataset is split into K number of folds or sections. Each of the K folds is considered as testing set at different iterations. For this modelling, we have used 5 fold cross-validation.

The best combination of the hyperparameters is chosen.

<div class="fig figcenter fighighlight"><img class="alignnone size-full wp-image-94" src="https://vkwritings.files.wordpress.com/2020/01/20.png" alt="20" width="721" height="274" /></div>

The results are sorted based on the scores of the test and the best combination is picked.

The table below displays the results and it is clear that the ‘max depth’ of 13 gives the best accuracy

<div class="fig figcenter fighighlight"><img class="alignnone size-full wp-image-95" src="https://vkwritings.files.wordpress.com/2020/01/22.png" alt="22" width="941" height="276" /></div>

We could have a look at the features which play an important part in this model.

<hr />

<div class="fig figcenter fighighlight"><img class="alignnone size-full wp-image-96" src="https://vkwritings.files.wordpress.com/2020/01/23.png" alt="23" width="790" height="166" /></div>
	      It can be seen that profile_state and Goal_in USD are 2 columns which have the most influence. One can also see the categories and subcategories which are highly influential.

<div class="fig figcenter fighighlight"><img class="alignnone size-full wp-image-97" src="https://vkwritings.files.wordpress.com/2020/01/24.png" alt="24" width="563" height="471" /></div>
<h3>LOGISTIC REGRESSION</h3>
Similar to the Decision Tree, we build a Logistic Regression model too. We use GridSearch and provide certain hyperparameters and chose the combination which gives the best result

<div class="fig figcenter fighighlight"><img class="alignnone size-full wp-image-98" src="https://vkwritings.files.wordpress.com/2020/01/25.png" alt="25" width="941" height="261" /></div>

As can be seen from the results, the best results are for a value of C = 4

<div class="fig figcenter fighighlight"><img class="alignnone size-full wp-image-104" src="https://vkwritings.files.wordpress.com/2020/01/32.png" alt="32" width="941" height="115" /></div>

Logistic Regression seems to give better results than the Decision tree, though the accuracy is only 76%.
	      
	      We could further preprocess by applying a PCA of the columns which have very high variance.
	      We could extract more columns and scale them using another scaler.
	      The process of preprocessing and hyper tuning the parameters is an iterative process.
	      Care has to taken not to overfit the model.<br>

<br>To view the whole code please use the link <a href="https://github.com/anavidya/kickstarter">Kickstarter</a><br>
	      To improve the results of the Machine learning solution one can
<ul>
	<li>Try different algorithms</li>
	<li>Look into features that can be used/dropped and feature engineering</li>
	<li>Tune the hyperparameters of the algorithm selected.</li>
</ul></p>
	      </div>
	      
	      <div id = "parkinsons">
		<h2>ML Model to detect Parkinsons</h2>
		<p><a href="https://www.parkinsons.org.uk/information-and-support/what-parkinsons/">Parkinsons</a> is a nuerodegerative 
		disorder in which a part of the 
		brain gets damaged progressively over years. <br>
		More than 1 million people in UK are affected by Parkinsons. People suffering from Parkinson's have not enough dopamine because the
		nerve cells producing them have been damaged.
	      	<p>The main steps that are followed for building a predictive model are</p>
		<p><ul>
		<li>Extracting data</li>
		<li>Exploratory data analysis</li>
		<li>Feature engineering</li>
		<li>Building a baseline model</li>
		<li>Building different models</li>
		<li>Comparing the metrics to check if model gives best result</li>  
		</ul></p>
	      	<h3><span style="color: #800000;">Main Objective</span></h3>
	      	<p> To build a model which can accurately detect if a person has PD (parkinson's) or not. The dataset used is from <a 
		href = "https://archive.ics.uci.edu/UCI">UCI ( Center for Machine Learning and Intelligent Systems).</a>
		The dataset was created by Max Little of the University of Oxford, in collaboration with the National 
			Centre for Voice and Speech, Denver, Colorado, who recorded the speech signals of many PD patients.
			Use this <a href ="https://archive.ics.uci.edu/ml/datasets/parkinsons">link</a>
			to download the data and learn further details
		The data consistes of voice recordings of 31 people out of which 23 are Parkinson's patients.
		The dataset is composed of a range of biomedical voice measurements. Each column in the table is a 
		particular voice measure, and each row corresponds one of 195 voice recording from these individuals ("name" column). 
		The main aim of the data is to discriminate healthy people from those with PD, 
		according to "status" column which is set to 0 for healthy and 1 for PD. 
	      <h3>Information of the features</h3></p>
	      <p>Attribute Information:
		<ul>
			<li>Matrix column entries (attributes): </li>
	<li>name - ASCII subject name and recording number </li>
	<li>MDVP:Fo(Hz) - Average vocal fundamental frequency </li>
	<li>MDVP:Fhi(Hz) - Maximum vocal fundamental frequency </li>
	<li>MDVP:Flo(Hz) - Minimum vocal fundamental frequency </li>
	<li>MDVP:Jitter(%),MDVP:Jitter(Abs),MDVP:RAP,MDVP:PPQ,Jitter:DDP - Several measures of variation in fundamental frequency </li>
	<li>MDVP:Shimmer,MDVP:Shimmer(dB),Shimmer:APQ3,Shimmer:APQ5,MDVP:APQ,Shimmer:DDA - Several measures of variation in amplitude </li>
	<li>NHR,HNR - Two measures of ratio of noise to tonal components in the voice </li>
	<li>status - Health status of the subject (one) - Parkinson's, (zero) - healthy </li>
	<li>RPDE,D2 - Two nonlinear dynamical complexity measures </li>
	<li>DFA - Signal fractal scaling exponent </li>
	<li>spread1,spread2,PPE - Three nonlinear measures of fundamental frequency variation 
	      </ul>
	      </p>	
	  	<h2><span style="color: #800000;">Reading and uploading data</span></h2>
	 <p>
		Python is the languaged used to analyse and build the model.The following libraries are imported
		<ul>
		<li>pandas: for easy manipulation of data structures.</li>
		<li>NumPy: For scientific computations</li>
		<li>sklearn : The machine learning library of Python</li>
		<li>xgboost : To create a Boosting Model</li>	
		<li>mlxtend :The library for builiding a stacking classifier</li>
		<li>matplotlib : Plotting graphs od data and results to visualise and understand  better</li>
		</ul>
	 <div class="fig figcenter fighighlight">
	<img class="alignnone size-full wp-image-87" src="images/lib1.PNG" 
	     alt="lib" width="360" height="82" />
	<img class="alignnone size-full wp-image-87" src="images/lib2.png" 
	     alt="lib" width="360" height="82" />
	 </div>
	 </p>
	<p>The data used for this evaluation is 
		stored as a CSV file format. It does contain a header and comma is used to seperate the various features.
		To begin with, read the CSV file and convert the data into a pandas dataframe.</br></p>
	<p>The data is inspected.The various features/attributes is inspected. There  are 24 columns. 
		The column 'name' which is used to identify the patient
		adds no value in training the model. The column named 'status' is the target which we need to predict. It can have 2 values '1' if
		the patient has PD, 0 if healthy.
		So we could assign 'y', the target pandas series as the column 'status'. All the columns except 'name' could be assigned to 'X'.
	<br>
	</p>
	<p>
	<h3> Feature Engineering</h3>
	There are no columns with categorical values . So there is no need to One-hot encode or label encode any feature. <br>
	The features have varying minimum
	and maximum values. The reason could be that the features have different magnitude or units. Most of the ML models are not interested in the units or magnitude .Obviously, the features with
	higher magnitude might weigh more than thoses with lesser magnitude. If we are to use algorithms which computes distance between feature
	point then scaling is a must.K-Nearest neigbhors is sensitive to the distance between feature points where as Tree-based models 
	do not care about the distance.
	<br>
	Upon inspecting the 'status column it can be noticed that there is a imbalance in the distribution of values. 75% of the people observed have PD
	wheare as 25% are marked as healthy. There is a slight class imbalance. Care must taken to ensure that a healthy person is not
	misclassified as 'PD' since the status value '1'(PD) is the dominant class. 
	This causes unecessary heartache for a healthy patient as well as the model will become unrealiable. 
	Since PD is not a minority class, wrong classification will not cost a life.<br>
	
	'Accuracy' cannot be used as the scoring strategy in this case. We could oversampling the minority class but that might lead to overfitting. Undersampling the majority class might 
	lead to model being biased.
	The ROC AUC scores could be used to compare and select a model.The ROC curve evaluates the probablity predictions made by the model on the test dataset.
	It is a plot of the flase-positive rate to the true positive rate.Thus helps to undertand the tradeoffs.
	Geometric-Mean or G-mean could also be used as a mteric to seek the balance between sensitivity and specificity.Precision-Recall curve couls also be used
	to check the performance of the classifier on the positive class.
	<br>
	The hyperparameter 'class-weight' is set as 'balanced' so that less weight is put on the majority class
	<br> n_samples/ (n_calss *np.bincount(y))
	
	</p>
	<h3>Split the data</h3>
	<p>
		The data is generallly split into training and testing sets. Around 20% is set aside as test data.
		
	</p>
	<h3>Baseline Model</h3>
	<p> Let us consdier to first use Logistic regression as the classifier to create a baseline model. This is not optimised
		and will be used to compare with other models.
	</p>
	<h3>Ensemble Model</h3>
	<p> When several ML techniques are combined together it is called as ensemble and this improves the accuravy of the predictions 
		sometimes. They are combined to reduce the variance(bagging), bias(boosting) and predictions accuracy(stacking).
		<br> Ensemble models are grouped as sequential or parallel. If the base learners are generated sequentially and each learns from the previous models 
		it s grouped as sequential.The weghts of the mislabbelled instances are boosted so that the next sequenetial model learns from it.
		In parallel, the base learners are independent of  each other. Bt averaging, the errors are reduced.
		<br></p>
<h4>Bagging</h4>
<p>
	Bagging stands for Bootstarp aggregating. Bootstarp samples are used to obtain various data susets to train the base learners.
	By avaraging the multiple estimates from the various base learners ,the overall variance of the estimate is reduced.
	<br>
	Random forest is one of the common algorith in ensemble. Instead of using all the features, a random subset of features is selectes , 
	thus further randomising the tree. Though bias might increase, the variance is decreased.
	
</p>
<h4>Boosting</h4>
<p>Several weak learners are better than random guessing. The misclassified instances are given more weight and trained again. Finally the weighted sum or 
	weighted majority vote is used to decide on the final prediction
</p>
<h3> Comparing the models</h3>
<p>Since there is a slight class imbalance, Classification report or accuracy score is not the metrics to be used to compare this models.
	
	This is a binary classification having labels as 0 or 1 . The default threshold for the probablity is 0.5.
	The value of probablities less than the threshold of 0.5 are assigned to class 0 and values greater than or equal to 0.5 are assigned to class 1.
	The class distribution for this dataset is slightly skewed and the cost of one type of misclassification 
	is more important than another type of misclassification. So the optimal threshold has to be chosen. 
	ROC curves can be used to analyse the predicted probablities of the model and the ROC-AUC curves could be used to select the best model.
	
	The curve gives an picture of the true and fale positive rates for the various thresholds.The area under the curve gives the accuracy of the model.


	If we adopt the boosting model then the optimum threshold could be set as 0.71
	<ul>
		<li>prediction < 0.71 = Class 0</li>
		<li>prediction >= 0.71 = Class 1</li>
	</ul.
</p>
	<h3>Understanding the importance of the features</h3>
		<p>
			Since Boosting seems to give the best score, if we decide to using a boosting model, xgboost classifier is able to show which attributes where considered important by the model 
			A plot of the importances of features with weights for each feature is displayed .The weights sum to one.
		</p>

	  </div>
	      </article>
	</div>
	</div>
	</div>

</body>
</html>
