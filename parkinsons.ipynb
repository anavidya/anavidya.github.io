%matplotlib inline
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn import preprocessing, model_selection
from mlxtend.classifier import StackingCVClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import GridSearchCV

#Read dataset from the csv file
data = pd.read_csv('data/parkinsons.data', sep=',')
y = data['status']
X = data.drop(['name','status'],axis = 1)

#Check to see the distribution of the target. Check for any class imbalance
status_map = {0:'Healthy', 1: 'PD' }
pd.Series(y).map(status_map).value_counts()
print('% of  healthy :{} % with PD :{}' .format((sum(y==0)/len(y)), sum(y==1)/len(y)))

# data does not have same range. This could be due to magnitude or units of the features. so scale them.
from sklearn.preprocessing import MinMaxScaler
cols = X.columns
scaler = MinMaxScaler(feature_range=(-1, 1))
X = pd.DataFrame(scaler.fit_transform(X), columns = cols)

# Split the dataset into the training set and test set
xTrain, xTest, yTrain, yTest = train_test_split(X, y, test_size = 0.2, random_state = 0)

#a basic logistic model
clf = LogisticRegression(class_weight = 'balanced', solver ='lbfgs')
clf.fit(xTrain,yTrain)
ypred_proba = clf.predict_proba(xTest)
ypred = clf.predict(xTest)

# display a report on the model
from sklearn.metrics import classification_report, roc_curve, confusion_matrix

print("Simple RF (baseline):")
print(classification_report(yTest, ypred))
fpr, tpr, thresholds = roc_curve(yTest, ypred_proba[:,1])
disp = confusion_matrix(yTest,ypred)

#lets plot the ROC curve and see the optimal threshold
gmeans = np.sqrt(tpr *(1-fpr))
ix = np.argmax(gmeans)
print('Best Threshold =%f ,G-means =%f'%(thresholds[ix], gmeans[ix]))
plt.plot(fpr, tpr, marker ='.', label ='Logistic')
plt.scatter(fpr[ix],tpr[ix],marker ='o',color ='red', label ='Best')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.legend()
plt.show()

#Lets stack and see the result
l0_rf = RandomForestClassifier()
l0_knn = KNeighborsClassifier()
l0_gb = GaussianNB()
l1_lr = LogisticRegression(penalty = 'l1', class_weight = 'balanced')
stack = StackingCVClassifier(classifiers=[l0_rf, l0_knn, l0_gb],
                           meta_classifier=l1_lr,
                           random_state=42,
                           use_features_in_secondary=True)
                           
                           from sklearn.metrics import classification_report
stack.fit(xTrain,yTrain)
ypred = stack.predict(xTest)
ypred_proba = stack.predict_proba(xTest)
print("Stacking:")
print(classification_report(yTest, ypred))

fpr, tpr, thresholds = roc_curve(yTest, ypred_proba[:,1])
gmeans = np.sqrt(tpr *(1-fpr))
ix = np.argmax(gmeans)
print('Best Threshold =%f ,G-means =%f'%(thresholds[ix], gmeans[ix]))
plt.plot(fpr, tpr, marker ='.', label ='stacking')
plt.scatter(fpr[ix],tpr[ix],marker ='o',color ='red', label ='Best')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.legend()
plt.show()

#Lets apply crossvalidation with a CV of 10 and check the results
params = {'kneighborsclassifier__n_neighbors': [1, 5],
          'randomforestclassifier__n_estimators': [100, 150]}

gcv = GridSearchCV(estimator=stack, 
                   param_grid=params, 
                   cv=10, n_jobs=-1)
                   
                   gcv.fit(xTrain,yTrain)
ypred = gcv.predict(xTest)
ypred_proba = gcv.predict_proba(xTest)
print("Stacking with CV")
print(classification_report(yTest, ypred))

fpr, tpr, thresholds = roc_curve(yTest, ypred_proba[:,1])
gmeans = np.sqrt(tpr *(1-fpr))
ix = np.argmax(gmeans)
print('Best Threshold =%f ,G-means =%f'%(thresholds[ix], gmeans[ix]))
plt.plot(fpr, tpr, marker ='.', label ='stacking with CV')
plt.scatter(fpr[ix],tpr[ix],marker ='o',color ='red', label ='Best')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.legend()
plt.show()

#build a boosting model and check the ROC-AUC
import xgboost as xgb
cv_params ={'max_depth': [5, 7], 'min_child_weight': [1, 3], 'n_estimators': [50, 100]}
ind_params ={'learning_rate': 0.1, 'random_state':0, 'subsample': 0.8, 'colsample_bytree': 0.8, 
              'objective': 'binary:logistic'}
optimised_xgb = GridSearchCV(estimator =xgb.XGBClassifier(**ind_params) , 
                             param_grid = cv_params, cv = 10, scoring = 'roc_auc', n_jobs = -1)
                             
                             optimised_xgb.fit(xTrain,yTrain)
ypred = optimised_xgb.predict(xTest)
ypred_proba = optimised_xgb.predict_proba(xTest)
print("Boosting with XGBboost")
print(classification_report(yTest, ypred))

fpr, tpr, thresholds = roc_curve(yTest, ypred_proba[:,1])
gmeans = np.sqrt(tpr *(1-fpr))
ix = np.argmax(gmeans)
print('Best Threshold =%f ,G-means =%f'%(thresholds[ix], gmeans[ix]))
plt.plot(fpr, tpr, marker ='.', label ='Boosting')
plt.scatter(fpr[ix],tpr[ix],marker ='o',color ='red', label ='Best')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.legend()
plt.show()

#plot the feature importance to see if the features selected by model really are important .
plt.figure(figsize=(20,15))
xgb.plot_importance(optimised_xgb.best_estimator_, ax=plt.gca())


